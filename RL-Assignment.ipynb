{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# base code from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training\n",
    "\n",
    "import gym\n",
    "import gym.wrappers.monitoring.video_recorder\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from  collections import deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import atari_wrappers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as Tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_printoptions(edgeitems=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# env = gym.make('Breakout-v4')#.unwrapped\n",
    "# env = gym.make('Gravitar-v0').unwrapped\n",
    "# game = 'BreakoutNoFrameskip-v4'\n",
    "game = 'GravitarNoFrameskip-v0'\n",
    "# game= 'PongNoFrameskip-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# Wrappers\n",
    "video_every=5\n",
    "env = atari_wrappers.make_atari(game)\n",
    "# env = gym.wrappers.Monitor(env, \"./recordings/\"+game+'/', video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
    "env = atari_wrappers.wrap_deepmind(env,episode_life=False,clip_rewards=False,frame_stack=True)\n",
    "# env = atari_wrappers.wrap_deepmind(env,episode_life=True,clip_rewards=False,frame_stack=True)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/10000 [1:10:33<2939:02:48, 1058.48s/it]\n",
      "  0%|          | 0/10000 [1:05:33<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Replay memory class, storing states, actions, following state and the reward gained\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "Sequence = namedtuple('Sequence',('transitions','recurrent_state'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity, files):\n",
    "        self.files = files\n",
    "        self.currNewestFile=0\n",
    "        self.fileName='./memory/memory-data'#+file+'.pt'\n",
    "        self.liveFiles = set()\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.priorities = np.zeros(capacity)\n",
    "        self.alpha = 0.6\n",
    "\n",
    "    def push(self, seq):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = seq\n",
    "        self.priorities[self.position] = 1\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        if self.position==0:\n",
    "            self.save()\n",
    "\n",
    "    def save(self):\n",
    "        memToSave = self.memory[:-BATCH_SIZE]\n",
    "        torch.save(memToSave,self.fileName+str(self.currNewestFile)+'.pt')\n",
    "        self.liveFiles.add(self.fileName+str(self.currNewestFile))\n",
    "        self.currNewestFile= (1+self.currNewestFile)%self.files\n",
    "        self.memory = self.memory[-BATCH_SIZE:]\n",
    "\n",
    "    def memSample(self):\n",
    "        probs = np.array(list(range(1,len(self.liveFiles)+2)))\n",
    "        probs[-1]*=2\n",
    "        probs= probs / probs.sum()\n",
    "\n",
    "        experiences = np.random.choice(len(self.liveFiles)+1,BATCH_SIZE,p=probs)\n",
    "        experiences = self.calcFreq(experiences)\n",
    "\n",
    "        data = []\n",
    "        for i in experiences:\n",
    "            # experiences[i] (batch size from that file), i (index of file)\n",
    "            if i==len(self.liveFiles):\n",
    "                data+= self.sample(experiences[i])\n",
    "            else:\n",
    "                x = (self.currNewestFile-i-1)%self.files\n",
    "                data+= self.loadFile(self.fileName+str(x)+'.pt',experiences[i])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def calcFreq(self,data):\n",
    "        freq = {}\n",
    "        for i in data:\n",
    "            if i in freq:\n",
    "                freq[i]+=1\n",
    "            else:\n",
    "                freq[i]=1\n",
    "        return freq\n",
    "    def loadFile(self,name,batch_size):\n",
    "        return random.sample(torch.load(name),batch_size)\n",
    "\n",
    "    # we have a local memory buffer with size capacity\n",
    "    # when it is full, save the whole memory to file 1, and delete capacity - batchsize data - leaving newest sequences\n",
    "    # repeat until all files are full - we clear oldest and work back\n",
    "    # probabilities are 1:2:3:4:...:files:files+1(local memory)\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def PERSample(self,batch_size, beta =0.6):\n",
    "        # beta chanegs over time\n",
    "        #Can change to use sum-tree\n",
    "        if len(self.memory) < self.capacity:\n",
    "            tmpPriorities = self.priorities[:self.position]\n",
    "        else:\n",
    "            tmpPriorities = self.priorities\n",
    "\n",
    "        #priority_j ^alpha / sum priorities\n",
    "        probs = tmpPriorities**self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
    "        tmpPriorities = tmpPriorities[indices]\n",
    "        loss_weights = (len(tmpPriorities)*tmpPriorities/len(self.memory))**(-beta)\n",
    "        loss_weights /= loss_weights.max()\n",
    "\n",
    "        return [self.memory[index] for index in indices], indices, loss_weights\n",
    "\n",
    "    def updatePriorities(self, indices, newPriorities):\n",
    "        for i in range(len(indices)):\n",
    "            self.priorities[indices[i]] = newPriorities[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/Shmuma/ptan/blob/master/samples/rainbow/lib/dqn_model.py\n",
    "# Factorised Noisy Layer\n",
    "class NoisyFactorizedLinear(nn.Linear):\n",
    "    \"\"\"\n",
    "    NoisyNet layer with factorized gaussian noise\n",
    "    N.B. nn.Linear already initializes weight and bias to\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, sigma_zero=0.4, bias=True):\n",
    "        super(NoisyFactorizedLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        sigma_init = sigma_zero / math.sqrt(in_features)\n",
    "        self.sigma_weight = nn.Parameter(torch.Tensor(out_features, in_features).fill_(sigma_init))\n",
    "        self.register_buffer(\"epsilon_input\", torch.zeros(1, in_features))\n",
    "        self.register_buffer(\"epsilon_output\", torch.zeros(out_features, 1))\n",
    "        if bias:\n",
    "            self.sigma_bias = nn.Parameter(torch.Tensor(out_features).fill_(sigma_init))\n",
    "\n",
    "    def forward(self, input):\n",
    "        torch.randn(self.epsilon_input.size(), out=self.epsilon_input)\n",
    "        torch.randn(self.epsilon_output.size(), out=self.epsilon_output)\n",
    "\n",
    "        func = lambda x: torch.sign(x) * torch.sqrt(torch.abs(x))\n",
    "        eps_in = func(self.epsilon_input)\n",
    "        eps_out = func(self.epsilon_output)\n",
    "\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias + self.sigma_bias * torch.autograd.Variable(eps_out.t())\n",
    "        noise_v = torch.autograd.Variable(torch.mul(eps_in, eps_out))\n",
    "        return F.linear(input, self.weight + self.sigma_weight * noise_v, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "# Number of Linear input connections depends on output of conv2d layers\n",
    "# and therefore the input image size, so compute it.\n",
    "def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class DuelingLSTMDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DuelingLSTMDQN, self).__init__()\n",
    "        if scaledScreenSize[1] > 80:\n",
    "            conv_out = 64\n",
    "            # Hyper parameters from Rainbow paper - https://arxiv.org/pdf/1710.02298.pdf\n",
    "            if grey:\n",
    "                self.conv1 = nn.Conv2d(1*4, 32, kernel_size=8, stride=4)\n",
    "            else:\n",
    "                self.conv1 = nn.Conv2d(3*4, 32, kernel_size=8, stride=4)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.conv3 = nn.Conv2d(64, conv_out, kernel_size=3, stride=1)\n",
    "            self.bn3 = nn.BatchNorm2d(conv_out)\n",
    "            convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w,kernel_size=8,stride=4),kernel_size=4),kernel_size=3,stride=1)\n",
    "            convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h,kernel_size=8,stride=4),kernel_size=4),kernel_size=3,stride=1)\n",
    "\n",
    "        else: # scaledScreenSize[0] == 64:\n",
    "            # for reduced model complexity\n",
    "            conv_out = 32\n",
    "            if grey:\n",
    "                self.conv1 = nn.Conv2d(1*4, 16, kernel_size=6, stride=3)\n",
    "            else:\n",
    "                self.conv1 = nn.Conv2d(3*4, 16, kernel_size=6, stride=3)\n",
    "            self.bn1 = nn.BatchNorm2d(16)\n",
    "            self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "            self.bn2 = nn.BatchNorm2d(32)\n",
    "            self.conv3 = nn.Conv2d(32, conv_out, kernel_size=3, stride=1)\n",
    "            self.bn3 = nn.BatchNorm2d(conv_out)\n",
    "            convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w,kernel_size=6,stride=3),kernel_size=4),kernel_size=3,stride=1)\n",
    "            convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h,kernel_size=6,stride=3),kernel_size=4),kernel_size=3,stride=1)\n",
    "\n",
    "\n",
    "        lstm_input_size = convw * convh * conv_out\n",
    "        self.linear_input_size = 512\n",
    "        # linear_input_size = convw * convh * conv_out\n",
    "\n",
    "\n",
    "        self.lstm = nn.LSTMCell(lstm_input_size, self.linear_input_size)\n",
    "        self.hiddenStatesBatch = torch.zeros(BATCH_SIZE,self.linear_input_size).to(device)\n",
    "        self.cellStatesBatch = torch.zeros(BATCH_SIZE,self.linear_input_size).to(device)\n",
    "        self.hiddenStatesLive = torch.zeros(1,self.linear_input_size).to(device)\n",
    "        self.cellStatesLive = torch.zeros(1,self.linear_input_size).to(device)\n",
    "\n",
    "        # dualing networks\n",
    "        # https://arxiv.org/pdf/1511.06581.pdf\n",
    "        self.adv1 = NoisyFactorizedLinear(self.linear_input_size, 512)\n",
    "        self.adv2 = NoisyFactorizedLinear(512, outputs)\n",
    "        self.val1 = NoisyFactorizedLinear(self.linear_input_size, 512)\n",
    "        self.val2 = NoisyFactorizedLinear(512, 1)\n",
    "        nn.init.kaiming_normal_(self.adv1.weight)\n",
    "        nn.init.kaiming_normal_(self.val1.weight)\n",
    "\n",
    "    def reset(self,done, batch=False):\n",
    "        if batch:\n",
    "            self.hiddenStatesBatch.detach()\n",
    "            self.cellStatesBatch.detach()\n",
    "            if done:\n",
    "                self.hiddenStatesBatch = self.hiddenStatesBatch.zero_()\n",
    "                self.cellStatesBatch = self.hiddenStatesBatch.zero_()\n",
    "        else:\n",
    "            self.hiddenStatesLive.detach()\n",
    "            self.cellStatesLive.detach()\n",
    "            if done:\n",
    "                self.hiddenStatesLive = self.hiddenStatesLive.zero_()\n",
    "                self.cellStatesLive = self.hiddenStatesLive.zero_()\n",
    "\n",
    "    def getState(self, batch=False):\n",
    "        if batch:\n",
    "            # print(self.hiddenStatesBatch.detach().clone().cpu())\n",
    "            # print( self.cellStatesBatch.detach().clone().cpu())\n",
    "            return self.hiddenStatesBatch.detach().clone().cpu(), self.cellStatesBatch.detach().clone().cpu()\n",
    "        else:\n",
    "            # print(self.hiddenStatesLive.detach().clone().cpu())\n",
    "            # print( self.cellStatesLive.detach().clone().cpu())\n",
    "            return self.hiddenStatesLive.detach().clone().cpu(), self.cellStatesLive.detach().clone().cpu()\n",
    "\n",
    "    def setState(self, state,device, batch=False):\n",
    "        if batch:\n",
    "            hiddens, cells = state\n",
    "            self.hiddenStatesBatch = hiddens.clone().to(device)\n",
    "            self.cellStatesBatch = cells.clone().to(device)\n",
    "        else:\n",
    "            hiddens, cells = state\n",
    "            self.hiddenStatesLive = hiddens.clone().to(device)\n",
    "            self.cellStatesLive = cells.clone().to(device)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x, hiddenState=None, cellState=None):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        if not(hiddenState==None):\n",
    "            # print(x.shape,hiddenState.shape,cellState.shape)\n",
    "            self.hiddenStatesLive, self.cellStatesLive = self.lstm(x,(hiddenState,cellState))\n",
    "            # print('Forward single ',self.hiddenStatesLive.shape)\n",
    "            # print(self.cellStatesLive.shape)\n",
    "            adv = F.relu(self.adv1(self.hiddenStatesLive))\n",
    "            val = F.relu(self.val1(self.hiddenStatesLive))\n",
    "\n",
    "        else:\n",
    "            # print(x.shape,self.hiddenStatesBatch.shape,self.cellStatesBatch.shape)\n",
    "            self.hiddenStatesBatch, self.cellStatesBatch = self.lstm(x,(self.hiddenStatesBatch,self.cellStatesBatch))\n",
    "            # print('Forward batch',self.hiddenStatesBatch.shape)\n",
    "            # print(self.cellStatesBatch.shape)\n",
    "            adv = F.relu(self.adv1(self.hiddenStatesBatch))\n",
    "            val = F.relu(self.val1(self.hiddenStatesBatch))\n",
    "\n",
    "        adv = self.adv2(adv)\n",
    "        val = self.val2(val)\n",
    "        return val + adv - adv.mean(1, keepdim=True)\n",
    "\n",
    "    def calcLoss(self,batch):\n",
    "        copy = DuelingLSTMDQN(screen_height, screen_width, n_actions).to(device)\n",
    "        copy.load_state_dict(self.state_dict())\n",
    "        copy.eval()\n",
    "        batch = Sequence(*zip(*batch))\n",
    "        batch = Sequence(list(zip(*batch.transitions)),list(zip(*batch.recurrent_state)))\n",
    "        # print(batch.recurrent_state[0])\n",
    "        # print(batch.recurrent_state[0])\n",
    "        # print(batch.recurrent_state[1])\n",
    "        # Needs checking\n",
    "        hiddenStates = torch.cat(batch.recurrent_state[0])\n",
    "        cellStates = torch.cat(batch.recurrent_state[1])\n",
    "\n",
    "        # print(hiddenStates)\n",
    "\n",
    "        self.setState((hiddenStates,cellStates),device,True)\n",
    "        target_net.setState((hiddenStates,cellStates),device,True)\n",
    "\n",
    "        #burn in the model\n",
    "        with torch.no_grad():\n",
    "            for t in range(n_burn_in):\n",
    "                tmpTransition = Transition(*zip(*batch.transitions[t]))\n",
    "                state = get_screen(tmpTransition.state,True).to(device)\n",
    "                self.forward(state)\n",
    "                target_net.forward(state)\n",
    "            copy.setState(self.getState(True),device,True)\n",
    "            for t in range(n_burn_in,n_burn_in+n_step_return):\n",
    "                tmpTransition = Transition(*zip(*batch.transitions[t]))\n",
    "                state = get_screen(tmpTransition.state,True).to(device)\n",
    "                copy.forward(state)\n",
    "                target_net.forward(state)\n",
    "        self.reset(False,True)\n",
    "        delta = torch.zeros(len(batch.transitions)-n_burn_in-n_step_return,BATCH_SIZE,1,device=device)\n",
    "        # print(delta.shape,len(batch.transitions)-n_burn_in-n_step_return,)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            for t in range(n_burn_in,len(batch.transitions)-n_step_return):\n",
    "                tmpTransition = Transition(*zip(*batch.transitions[t]))\n",
    "                nextTransition = Transition(*zip(*batch.transitions[t+n_step_return]))\n",
    "                # print(tmpTransition)\n",
    "\n",
    "                state_batch = get_screen(tmpTransition.state,True).to(device)\n",
    "                action_batch = torch.cat(tmpTransition.action).to(device)\n",
    "                reward_batch = torch.stack(tmpTransition.reward).to(device)\n",
    "                next_state_batch = get_screen(nextTransition.state,True).to(device)\n",
    "                done_batch = torch.stack(nextTransition.done).to(device)\n",
    "                # print(state_batch.shape,action_batch.shape,reward_batch.shape,next_state_batch.shape,done_batch.shape)\n",
    "                state_action_values = policy_net(state_batch)#.gather(1, action_batch)\n",
    "                # print(state_action_values,state_action_values.gather(1, action_batch))\n",
    "                state_action_values = state_action_values.gather(1,action_batch)\n",
    "                actionPrediction = copy.forward(next_state_batch).argmax(dim=1).unsqueeze(1).detach()\n",
    "                # print(actionPrediction)\n",
    "                next_state_values = target_net(next_state_batch).gather(1, actionPrediction).detach()\n",
    "                # print(next_state_values)\n",
    "                # print(done_batch,reward_batch)\n",
    "                expected_state_action_values = (self.inv_value_function(next_state_values) * GAMMA * (1.0 - done_batch)) \\\n",
    "                                           + reward_batch\n",
    "                expected_state_action_values = self.value_function(expected_state_action_values)\n",
    "\n",
    "                delta[t-n_burn_in] = F.smooth_l1_loss(state_action_values,expected_state_action_values.float(),reduction='none')\n",
    "\n",
    "        # 0.9 is eta value\n",
    "        # priorities = 0.9 * delta.max(dim=0)[0] + (1.0 - 0.9) * delta.mean(dim=0)\n",
    "        # change this to be squared and decrease learning rate\n",
    "        return delta.pow(2).sum(dim=0)#, priorities.detach()\n",
    "        # return delta.sum(dim=0) #, priorities.detach()\n",
    "\n",
    "    # https://arxiv.org/pdf/1805.11593.pdf\n",
    "    def value_function(self,x,eps=1e-3):\n",
    "        return x.sign() * ((x.abs()+1).sqrt()-1)+eps*x\n",
    "    def inv_value_function(self,x,eps=1e-3):\n",
    "        return x.sign() * ((((1+4*eps*(x.abs()+1+eps)).sqrt()-1) / (2 *eps)).pow(2)-1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaledScreenSize = (64,64)\n",
    "# scaledScreenSize = (110,84)\n",
    "grey = True\n",
    "if grey:\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                        # T.Resize(scaledScreenSize, interpolation=Image.CUBIC),\n",
    "                        # T.Grayscale(),\n",
    "                        T.ToTensor()])\n",
    "else:\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize(scaledScreenSize, interpolation=Image.CUBIC),\n",
    "                        T.ToTensor()])\n",
    "\n",
    "def get_screen(screen,batch=False):\n",
    "\n",
    "    temp = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    # BATCH,x,y,frame\n",
    "    if batch:\n",
    "        return torch.from_numpy(temp).permute(0,3,1,2)\n",
    "    else:\n",
    "        return torch.from_numpy(temp).unsqueeze(0).permute(0,3,1,2)\n",
    "\n",
    "\n",
    "# temp = env.reset()\n",
    "# env.step(0)\n",
    "# env.step(0)\n",
    "# plt.figure()\n",
    "# if grey:\n",
    "#     plt.imshow(get_screen(temp).cpu().permute(2,0,1)[0].numpy(),\n",
    "#             interpolation='none', cmap='gray')\n",
    "# else:\n",
    "#     plt.imshow(get_screen(temp).cpu().permute(1, 2, 0).numpy(),\n",
    "#             interpolation='none')\n",
    "# plt.title('What the model Sees')\n",
    "# plt.show()\n",
    "#\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "BATCH_SIZE = 16\n",
    "GAMMA = 0.999**3\n",
    "EPSILON_MAX=0.4\n",
    "# EPSILON_MAX=0.1\n",
    "EPSILON_MIN=0.01\n",
    "TARGET_UPDATE = 5 # episodes\n",
    "POLICY_UPDATE = 24 # frames\n",
    "BETA_DECAY = num_episodes/10\n",
    "EPS_DECAY=150\n",
    "OUTPUT_FREQUENCY = 10\n",
    "video_every=10\n",
    "n_step_return = 3\n",
    "n_sequence = 50 #size of LSTM\n",
    "n_overlap = 25 #the next RNN state is half of the sequence length\n",
    "n_burn_in = 30 #how many steps before RNN is set to actually update - ie gets better initial state\n",
    "\n",
    "# update to 20 batch - 50 n_sequence, eps decay 150, eps max 0.4, episodic end of life = true -\n",
    "# already made lr and loss power changes\n",
    "\n",
    "# init with correct dims\n",
    "if scaledScreenSize[1]==84:\n",
    "    screen_width = 84\n",
    "    screen_height = 84\n",
    "else:\n",
    "    screen_width = 64\n",
    "    screen_height = 64\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DuelingLSTMDQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DuelingLSTMDQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(),lr=7.5e-5,eps=1e-3)\n",
    "capacity = 500\n",
    "files = 10\n",
    "memory = ReplayMemory(capacity=capacity,files=files)\n",
    "\n",
    "\n",
    "def select_action(state,n_episode, LSTMState):\n",
    "    with torch.no_grad():\n",
    "        # t.max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was\n",
    "        # found, so we pick action with the larger expected reward.\n",
    "        x = policy_net(get_screen(state).to(device),LSTMState[0].to(device),LSTMState[1].to(device))\n",
    "        return x.max(1)[1].view(1, 1).cpu()\n",
    "    '''\n",
    "    # global steps_done\n",
    "    sample = random.random()\n",
    "\n",
    "    # reaches min in 2100 episodes\n",
    "    epsilon = max( EPSILON_MAX * math.exp(-(n_episode/EPS_DECAY)), EPSILON_MIN)\n",
    "    # gets to 30% in 600 episodes and starts at 0.8\n",
    "    # epsilon = max(EPSILON_MIN,EPSILON_MAX - EPSILON_MIN*(n_episode/EPS_DECAY))\n",
    "\n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            x = policy_net(get_screen(state).to(device),LSTMState[0].to(device),LSTMState[1].to(device))\n",
    "            return x.max(1)[1].view(1, 1).cpu()\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device='cpu', dtype=torch.long)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def optimize_model(n_episode):\n",
    "\n",
    "    beta = min(1, 0.4 + 1*(n_episode/BETA_DECAY))\n",
    "\n",
    "    if len(memory) < BATCH_SIZE*2:\n",
    "        return\n",
    "    lossArr = []\n",
    "    # BATCH SIZE is more like 32\n",
    "    for _ in range(2):\n",
    "\n",
    "        policy_net.reset(done=True,batch=True)\n",
    "        # transitions = memory.sample(BATCH_SIZE)\n",
    "        # transitions = memory.sample(BATCH_SIZE)\n",
    "        transitions = memory.memSample()\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        delta = policy_net.calcLoss(transitions)\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "        #                                       batch.next_state)), device=device, dtype=torch.bool)\n",
    "        # non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "        #                                             if s is not None]).to(device)\n",
    "        # state_batch = torch.cat(batch.state).to(device)\n",
    "        # action_batch = torch.cat(batch.action)\n",
    "        # reward_batch = torch.cat(batch.reward).to(device)\n",
    "        # done_batch = torch.cat(batch.done).to(device)\n",
    "        #\n",
    "        # # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # # columns of actions taken. These are the actions which would've been taken\n",
    "        # # for each batch state according to policy_net\n",
    "        #\n",
    "        # state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "        #\n",
    "        # # Compute V(s_{t+1}) for all next states.\n",
    "        # # Expected values of actions for non_final_next_states are computed based\n",
    "        # # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # # This is merged based on the mask, such that we'll have either the expected\n",
    "        # # state value or 0 in case the state was final.\n",
    "        # next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        #\n",
    "        # next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "        #\n",
    "        # # Compute the expected Q values\n",
    "        # expected_state_action_values = (next_state_values * GAMMA * (1.0 - done_batch)) \\\n",
    "        #                                + reward_batch\n",
    "        # # Compute Huber loss\n",
    "        # loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1), reduction='none')\n",
    "        # # add little epsilon to make it not have p(s) = 0\n",
    "        # priorities = (loss.abs() + 1e-5).detach()\n",
    "        # memory.updatePriorities(indices,priorities)\n",
    "        # loss = (delta * torch.tensor(np.expand_dims(loss_weights,1),dtype=torch.float).to(device)).mean().float()\n",
    "        loss = (delta).mean().float()\n",
    "        lossArr.append(loss.clone().detach().cpu().numpy())\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()\n",
    "    return lossArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10001 [00:29<44:57:30, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marking, episode: 0, score: 350.0, mean_score: 350.00, std_score: 0.00\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "seed = 742\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "marking=[]\n",
    "episodeScores=[]\n",
    "losses = []\n",
    "stepBuffer = deque(maxlen=n_step_return)\n",
    "#precalculate GAMMA n step return values\n",
    "gamma_nsteps = [GAMMA ** i for i in range(n_step_return+1)]\n",
    "sequenceBuffer = []\n",
    "recurrentBuffer = []\n",
    "frames = 0\n",
    "for n_episode in tqdm(range(num_episodes+1)):\n",
    "    # print('New Episode')\n",
    "    sequenceBuffer = []\n",
    "    recurrentBuffer = []\n",
    "    score=0\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    lives = env.ale.lives\n",
    "    while True:\n",
    "        recurrentBuffer.append(policy_net.getState())\n",
    "        # print(recurrentBuffer[-1][0].shape,recurrentBuffer[-1][1].shape)\n",
    "        # Select and perform an action\n",
    "        action = select_action(state,n_episode,recurrentBuffer[-1])\n",
    "\n",
    "        obvs, reward, done, info = env.step(action.item())\n",
    "        score+=reward\n",
    "\n",
    "        # Reward clipping and negative reward on death\n",
    "        # if info['ale.lives'] != lives:\n",
    "        #     # reward-=1\n",
    "        #     lives = info['ale.lives']\n",
    "        #     if lives==0:\n",
    "        #         episodeScores.append(int(score))\n",
    "        #         marking.append(score)\n",
    "        # reward = np.clip(reward, -1, 1)\n",
    "        reward = torch.tensor([reward])\n",
    "\n",
    "        # Observe new state\n",
    "        next_state = obvs\n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        stepBuffer.append(Transition(state,action,next_state,reward,torch.tensor([float(done)])))\n",
    "        if len(stepBuffer) == stepBuffer.maxlen:\n",
    "            reward_nstep = sum([gamma_nsteps[-(i+2)] * stepBuffer[i].reward for i in range(n_step_return)])\n",
    "            sequenceBuffer.append(Transition(stepBuffer[0].state,stepBuffer[0].action,stepBuffer[0].next_state,reward_nstep,torch.tensor([float(done)])))\n",
    "        if (len (sequenceBuffer)) == n_sequence+n_burn_in:\n",
    "            memory.push(Sequence(sequenceBuffer,recurrentBuffer[0]))\n",
    "            #chop sequence and recurrent buffers\n",
    "            # print(len(sequenceBuffer[-(n_burn_in+n_overlap):]),len(recurrentBuffer[-(n_burn_in+n_overlap+n_step_return-1):]))\n",
    "            sequenceBuffer = sequenceBuffer[-(n_burn_in+n_overlap):]\n",
    "            recurrentBuffer = recurrentBuffer[-(n_burn_in+n_overlap+n_step_return):]\n",
    "            # print(recurrentBuffer[0][0].shape,recurrentBuffer[0][1].shape)\n",
    "        elif done and len(sequenceBuffer) > n_burn_in+n_overlap+n_step_return:\n",
    "            # case of we're done so we push the rest of the sequence to memory\n",
    "            # print(recurrentBuffer[0][0].shape,recurrentBuffer[0][1].shape)\n",
    "            memory.push(Sequence(sequenceBuffer,recurrentBuffer[0]))\n",
    "\n",
    "        if frames % POLICY_UPDATE == 0:\n",
    "            x = optimize_model(n_episode)\n",
    "            if x != None:\n",
    "                losses += x\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        frames+=1\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            stepBuffer.clear()\n",
    "\n",
    "            break\n",
    "    episodeScores.append(int(score))\n",
    "    marking.append(score)\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if n_episode % TARGET_UPDATE == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    # do not change lines 44-48 here, they are for marking the submission log\n",
    "    if n_episode%100 == 0:\n",
    "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
    "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
    "        marking = []\n",
    "\n",
    "    if n_episode % OUTPUT_FREQUENCY == 0:\n",
    "        if len(marking)>0:\n",
    "            print(\"episode: {}, mean_last30: {:.2f}, mean_score: {:.2f}, std_score: {:.2f}, mean_loss: {:.4f}\".format(\n",
    "                n_episode, np.array(episodeScores[-30:]).mean(), np.array(marking).mean(), np.array(marking).std(),np.array(losses[-5:]).mean()),episodeScores[-OUTPUT_FREQUENCY:])\n",
    "\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trend Graphs\n",
    "episodeScoresEnum = sorted(enumerate(episodeScores),reverse=True,key=lambda x:x[1])\n",
    "print('Top 5 scores overall')\n",
    "print(episodeScoresEnum[:5])\n",
    "tempScores = sorted([(i,j) for i,j in enumerate(episodeScores) if i%video_every==0],key=lambda x:x[1],reverse=True)\n",
    "print('Top videos:')\n",
    "print(tempScores[:10])\n",
    "print('Bottom Videos')\n",
    "print(tempScores[-10:][::-1])\n",
    "print('Total Frames Trained on: ',frames)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "averages = []\n",
    "tempAverage = 0\n",
    "for i in enumerate(losses):\n",
    "    tempAverage+=i[1]\n",
    "    if i[0]%10==0:\n",
    "        averages.append(tempAverage/10)\n",
    "        tempAverage=0\n",
    "plt.plot(range(len(averages)),averages)\n",
    "plt.ylabel('Average Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('per 10 updates')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "averages30 = []\n",
    "averages = []\n",
    "\n",
    "for i in range(len(episodeScores)):\n",
    "    averages.append(np.array(episodeScores[i-100:i+1]).mean())\n",
    "    averages30.append(np.array(episodeScores[i-30:i+1]).mean())\n",
    "\n",
    "plt.plot(range(len(episodeScores)),episodeScores,alpha=0.4,label='actual')\n",
    "plt.plot(range(len(averages)),averages,label='100 average')\n",
    "plt.plot(range(len(averages30)),averages30,label='30 average',alpha=0.6)\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Average score')\n",
    "plt.xlabel('rolling average over 100 episodes')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}