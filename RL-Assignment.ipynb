{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# base code from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize((84,84), interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "def get_screen(screen):\n",
    "\n",
    "    # if action:\n",
    "    #     obvsTuple = env.step(action) #env.render(mode='rgb_array',).transpose((2, 0, 1))\n",
    "    #     screen = obvsTuple[0]\n",
    "    # else:\n",
    "    #     screen = env.reset()\n",
    "    # screen = cv2.resize(_screen, dsize=(84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "    screen_height, screen_width, channel = screen.shape\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    screen = screen.permute(2,0,1)\n",
    "\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "\n",
    "temp = env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen(temp).cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: VideoRecorder encoder exited with status 1\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "video_every=10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen(env.reset())\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 50 episode averages and plot them too\n",
    "    if len(durations_t) >= 50:\n",
    "        means = durations_t.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: VideoRecorder encoder exited with status 1\u001B[0m\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBrokenPipeError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-cf59bbdea245>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mselect_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m         \u001B[0mobvs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m         \u001B[0mreward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mreward\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/DLRL/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_before_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m         \u001B[0mdone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_after_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/DLRL/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001B[0m in \u001B[0;36m_after_step\u001B[0;34m(self, observation, reward, done, info)\u001B[0m\n\u001B[1;32m    169\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstats_recorder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mafter_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m         \u001B[0;31m# Record video\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvideo_recorder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcapture_frame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/DLRL/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001B[0m in \u001B[0;36mcapture_frame\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    114\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_encode_ansi_frame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 116\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_encode_image_frame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    117\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/DLRL/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001B[0m in \u001B[0;36m_encode_image_frame\u001B[0;34m(self, frame)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 166\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcapture_frame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    167\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0merror\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInvalidFrame\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Tried to pass invalid video frame, marking as broken: %s'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/DLRL/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001B[0m in \u001B[0;36mcapture_frame\u001B[0;34m(self, frame)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    303\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdistutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mversion\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLooseVersion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__version__\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mdistutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mversion\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLooseVersion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'1.9.0'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 304\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mproc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstdin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    305\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mproc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstdin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtostring\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mBrokenPipeError\u001B[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "video_every=5\n",
    "# env = gym.wrappers.Monitor(env, \"./recordings/breakout/\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
    "num_episodes = 150\n",
    "for i_episode in tqdm(range(num_episodes+1)):\n",
    "    # Initialize the environment and state\n",
    "    temp = env.reset()\n",
    "    last_screen = get_screen(temp)\n",
    "    current_screen = get_screen(temp)\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "\n",
    "        obvs, reward, done, info = env.step(action.item())\n",
    "        if reward != 0:\n",
    "          pass\n",
    "          # print(reward)\n",
    "          # plt.imshow(get_screen(obvs).cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
    "          # plt.show()\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen(obvs)\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}