{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# base code from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training\n",
    "\n",
    "import gym\n",
    "import gym.wrappers.monitoring.video_recorder\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from  collections import deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as Tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_printoptions(edgeitems=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v4')#.unwrapped\n",
    "# env = gym.make('Gravitar-v0').unwrapped\n",
    "\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
    "# openAI's Frame skip and max frame wrapper\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Wrappers\n",
    "stackSize = 4\n",
    "video_every=5\n",
    "env = gym.wrappers.Monitor(env, \"./recordings/breakout/\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
    "env = MaxAndSkipEnv(env)\n",
    "env = gym.wrappers.frame_stack.FrameStack(env,stackSize)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Replay memory class, storing states, actions, following state and the reward gained\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "Sequence = namedtuple('Sequence',('transitions','recurrent_state'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.priorities = np.zeros(capacity)\n",
    "        self.alpha = 0.6\n",
    "        self.memoryfull = False\n",
    "\n",
    "    def push(self, seq):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = seq\n",
    "        self.priorities[self.position] = 1\n",
    "        if self.position>capacity:\n",
    "            self.memoryfull=True\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def PERSample(self,batch_size, beta =0.6):\n",
    "        # beta chanegs over time\n",
    "        #Can change to use sum-tree\n",
    "        if len(self.memory) < self.capacity:\n",
    "            tmpPriorities = self.priorities[:self.position]\n",
    "        else:\n",
    "            tmpPriorities = self.priorities\n",
    "\n",
    "        #priority_j ^alpha / sum priorities\n",
    "        probs = tmpPriorities**self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
    "        if self.memoryfull:\n",
    "            loss_weights = (len(tmpPriorities)*tmpPriorities/self.capacity)**(-beta)\n",
    "        else:\n",
    "            loss_weights = (len(tmpPriorities)*tmpPriorities/len(memory))**(-beta)\n",
    "        loss_weights /= loss_weights.max()\n",
    "\n",
    "        return [self.memory[index] for index in indices], indices, loss_weights\n",
    "\n",
    "    def updatePriorities(self, indices, newPriorities):\n",
    "        for i in range(len(indices)):\n",
    "            self.priorities[indices[i]] = newPriorities[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Number of Linear input connections depends on output of conv2d layers\n",
    "# and therefore the input image size, so compute it.\n",
    "def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class DuelingLSTMDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DuelingLSTMDQN, self).__init__()\n",
    "        if scaledScreenSize[1] > 80:\n",
    "            conv_out = 64\n",
    "            # Hyper parameters from Rainbow paper - https://arxiv.org/pdf/1710.02298.pdf\n",
    "            if grey:\n",
    "                self.conv1 = nn.Conv2d(1*stackSize, 32, kernel_size=8, stride=4)\n",
    "            else:\n",
    "                self.conv1 = nn.Conv2d(3*stackSize, 32, kernel_size=8, stride=4)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.conv3 = nn.Conv2d(64, conv_out, kernel_size=3, stride=1)\n",
    "            self.bn3 = nn.BatchNorm2d(conv_out)\n",
    "            convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w,kernel_size=8,stride=4),kernel_size=4),kernel_size=3,stride=1)\n",
    "            convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h,kernel_size=8,stride=4),kernel_size=4),kernel_size=3,stride=1)\n",
    "\n",
    "        else: # scaledScreenSize[0] == 64:\n",
    "            # for reduced model complexity\n",
    "            conv_out = 32\n",
    "            if grey:\n",
    "                self.conv1 = nn.Conv2d(1*stackSize, 16, kernel_size=6, stride=3)\n",
    "            else:\n",
    "                self.conv1 = nn.Conv2d(3*stackSize, 16, kernel_size=6, stride=3)\n",
    "            self.bn1 = nn.BatchNorm2d(16)\n",
    "            self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "            self.bn2 = nn.BatchNorm2d(32)\n",
    "            self.conv3 = nn.Conv2d(32, conv_out, kernel_size=3, stride=1)\n",
    "            self.bn3 = nn.BatchNorm2d(conv_out)\n",
    "            convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w,kernel_size=6,stride=3),kernel_size=4),kernel_size=3,stride=1)\n",
    "            convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h,kernel_size=6,stride=3),kernel_size=4),kernel_size=3,stride=1)\n",
    "\n",
    "\n",
    "        lstm_input_size = convw * convh * conv_out\n",
    "        self.linear_input_size = 512\n",
    "        # linear_input_size = convw * convh * conv_out\n",
    "\n",
    "\n",
    "        self.lstm = nn.LSTMCell(lstm_input_size, self.linear_input_size)\n",
    "        self.hiddenStatesBatch = torch.zeros(BATCH_SIZE,self.linear_input_size).to(device)\n",
    "        self.cellStatesBatch = torch.zeros(BATCH_SIZE,self.linear_input_size).to(device)\n",
    "        self.hiddenStatesLive = torch.zeros(1,self.linear_input_size).to(device)\n",
    "        self.cellStatesLive = torch.zeros(1,self.linear_input_size).to(device)\n",
    "        # print(self.hiddenStates)\n",
    "        # print(self.hiddenStates.shape)\n",
    "        # print(self.cellStates)\n",
    "        # print(self.cellStates.shape)\n",
    "\n",
    "        # dualing networks\n",
    "        # https://arxiv.org/pdf/1511.06581.pdf\n",
    "        self.adv1 = nn.Linear(self.linear_input_size, 512)\n",
    "        self.adv2 = nn.Linear(512, outputs)\n",
    "        self.val1 = nn.Linear(self.linear_input_size, 512)\n",
    "        self.val2 = nn.Linear(512, 1)\n",
    "        nn.init.kaiming_normal_(self.adv1.weight)\n",
    "        nn.init.kaiming_normal_(self.val1.weight)\n",
    "\n",
    "    def reset(self,done, batch=False):\n",
    "        if batch:\n",
    "            self.hiddenStatesBatch.detach()\n",
    "            self.cellStatesBatch.detach()\n",
    "            if done:\n",
    "                self.hiddenStatesBatch = self.hiddenStatesBatch.zero_()\n",
    "                self.cellStatesBatch = self.hiddenStatesBatch.zero_()\n",
    "        else:\n",
    "            self.hiddenStatesLive.detach()\n",
    "            self.cellStatesLive.detach()\n",
    "            if done:\n",
    "                self.hiddenStatesLive = self.hiddenStatesLive.zero_()\n",
    "                self.cellStatesLive = self.hiddenStatesLive.zero_()\n",
    "\n",
    "    def getState(self, batch=False):\n",
    "        if batch:\n",
    "            # print(self.hiddenStatesBatch.detach().clone().cpu())\n",
    "            # print( self.cellStatesBatch.detach().clone().cpu())\n",
    "            return self.hiddenStatesBatch.detach().clone().cpu(), self.cellStatesBatch.detach().clone().cpu()\n",
    "        else:\n",
    "            # print(self.hiddenStatesLive.detach().clone().cpu())\n",
    "            # print( self.cellStatesLive.detach().clone().cpu())\n",
    "            return self.hiddenStatesLive.detach().clone().cpu(), self.cellStatesLive.detach().clone().cpu()\n",
    "\n",
    "    def setState(self, state,device, batch=False):\n",
    "        if batch:\n",
    "            hiddens, cells = state\n",
    "            self.hiddenStatesBatch = hiddens.clone().to(device)\n",
    "            self.cellStatesBatch = cells.clone().to(device)\n",
    "        else:\n",
    "            hiddens, cells = state\n",
    "            self.hiddenStatesLive = hiddens.clone().to(device)\n",
    "            self.cellStatesLive = cells.clone().to(device)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x, hiddenState=None, cellState=None):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if not(hiddenState==None):\n",
    "            # print(x.shape,hiddenState.shape,cellState.shape)\n",
    "            self.hiddenStatesLive, self.cellStatesLive = self.lstm(x,(hiddenState,cellState))\n",
    "            # print('Forward single ',self.hiddenStatesLive.shape)\n",
    "            # print(self.cellStatesLive.shape)\n",
    "            adv = F.relu(self.adv1(self.hiddenStatesLive))\n",
    "            val = F.relu(self.val1(self.hiddenStatesLive))\n",
    "\n",
    "        else:\n",
    "            # print(x.shape,self.hiddenStatesBatch.shape,self.cellStatesBatch.shape)\n",
    "            self.hiddenStatesBatch, self.cellStatesBatch = self.lstm(x,(self.hiddenStatesBatch,self.cellStatesBatch))\n",
    "            # print('Forward batch',self.hiddenStatesBatch.shape)\n",
    "            # print(self.cellStatesBatch.shape)\n",
    "            adv = F.relu(self.adv1(self.hiddenStatesBatch))\n",
    "            val = F.relu(self.val1(self.hiddenStatesBatch))\n",
    "\n",
    "        adv = self.adv2(adv)\n",
    "        val = self.val2(val)\n",
    "        return val + adv - adv.mean(1, keepdim=True)\n",
    "\n",
    "    def calcLoss(self,batch):\n",
    "        copy = DuelingLSTMDQN(screen_height, screen_width, n_actions).to(device)\n",
    "        copy.load_state_dict(self.state_dict())\n",
    "        copy.eval()\n",
    "        batch = Sequence(*zip(*batch))\n",
    "        batch = Sequence(list(zip(*batch.transitions)),list(zip(*batch.recurrent_state)))\n",
    "        # print(batch.recurrent_state[0])\n",
    "        # print(batch.recurrent_state[0])\n",
    "        # print(batch.recurrent_state[1])\n",
    "        # Needs checking\n",
    "        hiddenStates = torch.cat(batch.recurrent_state[0])\n",
    "        cellStates = torch.cat(batch.recurrent_state[1])\n",
    "\n",
    "        # print(hiddenStates)\n",
    "\n",
    "        self.setState((hiddenStates,cellStates),device,True)\n",
    "        target_net.setState((hiddenStates,cellStates),device,True)\n",
    "\n",
    "        #burn in the model\n",
    "        with torch.no_grad():\n",
    "            for t in range(n_burn_in):\n",
    "                tmpTransition = Transition(*zip(*batch.transitions[t]))\n",
    "                state = torch.cat(tmpTransition.state).to(device)\n",
    "                # print(state.shape,t,self.getState())\n",
    "                self.forward(state)\n",
    "                target_net.forward(state)\n",
    "            copy.setState(self.getState(True),device,True)\n",
    "            for t in range(n_burn_in,n_burn_in+n_step_return):\n",
    "                tmpTransition = Transition(*zip(*batch.transitions[t]))\n",
    "                state = torch.cat(tmpTransition.state).to(device)\n",
    "                copy.forward(state)\n",
    "                target_net.forward(state)\n",
    "        # print(np.array(batch.transitions).shape)\n",
    "        self.reset(False,True)\n",
    "        delta = torch.zeros(len(batch.transitions)-n_burn_in-n_step_return,BATCH_SIZE,1,device=device)\n",
    "        # print(delta.shape,len(batch.transitions)-n_burn_in-n_step_return,)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            for t in range(n_burn_in,len(batch.transitions)-n_step_return):\n",
    "                tmpTransition = Transition(*zip(*batch.transitions[t]))\n",
    "                nextTransition = Transition(*zip(*batch.transitions[t+n_step_return]))\n",
    "                # print(tmpTransition)\n",
    "\n",
    "                state_batch = torch.cat(tmpTransition.state).to(device)\n",
    "                action_batch = torch.cat(tmpTransition.action).to(device)\n",
    "                reward_batch = torch.stack(tmpTransition.reward).to(device)\n",
    "                next_state_batch = torch.cat(nextTransition.state).to(device)\n",
    "                done_batch = torch.stack(nextTransition.done).to(device)\n",
    "                # print(state_batch.shape,action_batch.shape,reward_batch.shape,next_state_batch.shape,done_batch.shape)\n",
    "                state_action_values = policy_net(state_batch)#.gather(1, action_batch)\n",
    "                # print(state_action_values,state_action_values.gather(1, action_batch))\n",
    "                state_action_values = state_action_values.gather(1,action_batch)\n",
    "                actionPrediction = copy.forward(next_state_batch).argmax(dim=1).unsqueeze(1).detach()\n",
    "                # print(actionPrediction)\n",
    "                next_state_values = target_net(next_state_batch).gather(1, actionPrediction).detach()\n",
    "                # print(next_state_values)\n",
    "                # print(done_batch,reward_batch)\n",
    "                expected_state_action_values = (next_state_values * GAMMA * (1.0 - done_batch)) \\\n",
    "                                           + reward_batch\n",
    "                # print(expected_state_action_values)\n",
    "                # print(F.smooth_l1_loss(state_action_values,expected_state_action_values.float(),reduction='none'))\n",
    "                # print(state_action_values,next_state_values)\n",
    "                # print(F.smooth_l1_loss(state_action_values,expected_state_action_values.float(),reduction='mean'))\n",
    "                delta[t-n_burn_in] = F.smooth_l1_loss(state_action_values,expected_state_action_values.float(),reduction='none')\n",
    "\n",
    "        # 0.9 is eta value\n",
    "        priorities = 0.9 * delta.max(dim=0)[0] + (1.0 - 0.9) * delta.mean(dim=0)\n",
    "        return delta.pow(2).sum(dim=0), priorities.detach()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaledScreenSize = (84,64)\n",
    "grey = True\n",
    "if grey:\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize(scaledScreenSize, interpolation=Image.CUBIC),\n",
    "                        T.Grayscale(),\n",
    "                        T.ToTensor()])\n",
    "else:\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize(scaledScreenSize, interpolation=Image.CUBIC),\n",
    "                        T.ToTensor()])\n",
    "\n",
    "def get_screen(screen):\n",
    "    final =[]\n",
    "    for i in range(len(screen)):\n",
    "\n",
    "        ## Need to convert data type to 32F\n",
    "        temp = np.ascontiguousarray(screen[i], dtype=np.float32) / 255\n",
    "        temp = torch.from_numpy(np.array(temp)).permute(2,0,1)\n",
    "\n",
    "        temp = resize(temp)\n",
    "\n",
    "        # needs changing\n",
    "        temp = Tf.crop(temp,16,0,64,64)\n",
    "\n",
    "        final.append(temp.unsqueeze(0))\n",
    "\n",
    "    final = torch.cat(final).permute(1,0,2,3)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "# temp = env.reset()\n",
    "# plt.figure()\n",
    "# if grey:\n",
    "#     plt.imshow(get_screen(temp).cpu().permute(1, 2, 3, 0)[0].numpy(),\n",
    "#             interpolation='none', cmap='gray')\n",
    "# else:\n",
    "#     plt.imshow(get_screen(temp).cpu().permute(1, 2, 0).numpy(),\n",
    "#             interpolation='none')\n",
    "# plt.title('What the model Sees')\n",
    "# plt.show()\n",
    "#\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "BATCH_SIZE = 20\n",
    "GAMMA = 0.999**3\n",
    "EPSILON_MAX=0.3\n",
    "# EPSILON_MAX=0.1\n",
    "EPSILON_MIN=0.01\n",
    "TARGET_UPDATE = 2500\n",
    "POLICY_UPDATE = 12\n",
    "EPS_DECAY = num_episodes/4\n",
    "OUTPUT_FREQUENCY = 5\n",
    "video_every=10\n",
    "n_step_return = 3\n",
    "n_sequence = 60 #size of LSTM\n",
    "n_overlap = 30 #the next RNN state is half of the sequence length\n",
    "n_burn_in = 30 #how many steps before RNN is set to actually update - ie gets better initial state\n",
    "\n",
    "# init with correct dims\n",
    "if scaledScreenSize[1]==84:\n",
    "    screen_width = 84\n",
    "    screen_height = 84\n",
    "else:\n",
    "    screen_width = 64\n",
    "    screen_height = 64\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DuelingLSTMDQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DuelingLSTMDQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(),lr=1e-4,eps=1e-3)\n",
    "capacity = 150\n",
    "memory = ReplayMemory(capacity=capacity)\n",
    "\n",
    "\n",
    "def select_action(state,n_episode, LSTMState):\n",
    "    # global steps_done\n",
    "    sample = random.random()\n",
    "\n",
    "    # reaches min in 2100 episodes\n",
    "    epsilon = max( EPSILON_MAX * math.exp(-(n_episode/300)), EPSILON_MIN)\n",
    "    # gets to 30% in 600 episodes and starts at 0.8\n",
    "    # epsilon = max(EPSILON_MIN,EPSILON_MAX - EPSILON_MIN*(n_episode/EPS_DECAY))\n",
    "\n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            x = policy_net(state.to(device),LSTMState[0].to(device),LSTMState[1].to(device))\n",
    "            return x.max(1)[1].view(1, 1).cpu()\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device='cpu', dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def optimize_model(n_episode):\n",
    "\n",
    "    beta = min(1, 0.4 + 1*(n_episode/EPS_DECAY))\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    for _ in range(2):\n",
    "\n",
    "        policy_net.reset(done=True,batch=True)\n",
    "        # transitions = memory.sample(BATCH_SIZE)\n",
    "        transitions, indices, loss_weights = memory.PERSample(BATCH_SIZE,beta)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        delta, priorities = policy_net.calcLoss(transitions)\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "        #                                       batch.next_state)), device=device, dtype=torch.bool)\n",
    "        # non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "        #                                             if s is not None]).to(device)\n",
    "        # state_batch = torch.cat(batch.state).to(device)\n",
    "        # action_batch = torch.cat(batch.action)\n",
    "        # reward_batch = torch.cat(batch.reward).to(device)\n",
    "        # done_batch = torch.cat(batch.done).to(device)\n",
    "        #\n",
    "        # # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # # columns of actions taken. These are the actions which would've been taken\n",
    "        # # for each batch state according to policy_net\n",
    "        #\n",
    "        # state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "        #\n",
    "        # # Compute V(s_{t+1}) for all next states.\n",
    "        # # Expected values of actions for non_final_next_states are computed based\n",
    "        # # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # # This is merged based on the mask, such that we'll have either the expected\n",
    "        # # state value or 0 in case the state was final.\n",
    "        # next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        #\n",
    "        # next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "        #\n",
    "        # # Compute the expected Q values\n",
    "        # expected_state_action_values = (next_state_values * GAMMA * (1.0 - done_batch)) \\\n",
    "        #                                + reward_batch\n",
    "        # # Compute Huber loss\n",
    "        # loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1), reduction='none')\n",
    "        # # add little epsilon to make it not have p(s) = 0\n",
    "        # priorities = (loss.abs() + 1e-5).detach()\n",
    "        memory.updatePriorities(indices,priorities)\n",
    "\n",
    "        loss = (delta * torch.tensor(loss_weights,dtype=torch.float).to(device)).mean().float()\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 563/10001 [3:08:30<109:46:22, 41.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marking, episode: 0, score: 2.0, mean_score: 2.00, std_score: 0.00\n",
      "episode: 5, score: 0.0, mean_score: 1.60, std_score: 1.50\n",
      "episode: 10, score: 1.0, mean_score: 1.60, std_score: 1.20\n",
      "episode: 15, score: 4.0, mean_score: 1.87, std_score: 1.67\n",
      "episode: 20, score: 3.0, mean_score: 1.85, std_score: 1.62\n",
      "episode: 25, score: 3.0, mean_score: 1.92, std_score: 1.55\n",
      "episode: 30, score: 0.0, mean_score: 1.83, std_score: 1.55\n",
      "episode: 35, score: 5.0, mean_score: 1.94, std_score: 1.66\n",
      "episode: 40, score: 2.0, mean_score: 1.93, std_score: 1.62\n",
      "episode: 45, score: 2.0, mean_score: 1.80, std_score: 1.60\n",
      "episode: 50, score: 2.0, mean_score: 1.86, std_score: 1.60\n",
      "episode: 55, score: 3.0, mean_score: 1.84, std_score: 1.55\n",
      "episode: 60, score: 1.0, mean_score: 1.85, std_score: 1.54\n",
      "episode: 65, score: 3.0, mean_score: 2.02, std_score: 1.77\n",
      "episode: 70, score: 2.0, mean_score: 2.00, std_score: 1.72\n",
      "episode: 75, score: 2.0, mean_score: 1.96, std_score: 1.68\n",
      "episode: 80, score: 4.0, mean_score: 2.05, std_score: 1.68\n",
      "episode: 85, score: 1.0, mean_score: 2.16, std_score: 1.79\n",
      "episode: 90, score: 5.0, mean_score: 2.23, std_score: 1.82\n",
      "episode: 95, score: 1.0, mean_score: 2.27, std_score: 1.81\n",
      "marking, episode: 100, score: 5.0, mean_score: 2.32, std_score: 1.81\n",
      "episode: 105, score: 5.0, mean_score: 3.20, std_score: 1.47\n",
      "episode: 110, score: 2.0, mean_score: 2.90, std_score: 1.30\n",
      "episode: 115, score: 2.0, mean_score: 3.07, std_score: 1.34\n",
      "episode: 120, score: 5.0, mean_score: 3.05, std_score: 1.36\n",
      "episode: 125, score: 4.0, mean_score: 2.88, std_score: 1.34\n",
      "episode: 130, score: 4.0, mean_score: 3.00, std_score: 1.41\n",
      "episode: 135, score: 1.0, mean_score: 3.17, std_score: 1.73\n",
      "episode: 140, score: 5.0, mean_score: 3.30, std_score: 1.87\n",
      "episode: 145, score: 7.0, mean_score: 3.40, std_score: 1.88\n",
      "episode: 150, score: 3.0, mean_score: 3.56, std_score: 1.92\n",
      "episode: 155, score: 5.0, mean_score: 3.53, std_score: 1.89\n",
      "episode: 160, score: 3.0, mean_score: 3.55, std_score: 1.87\n",
      "episode: 165, score: 2.0, mean_score: 3.63, std_score: 1.93\n",
      "episode: 170, score: 6.0, mean_score: 3.70, std_score: 1.97\n",
      "episode: 175, score: 3.0, mean_score: 3.71, std_score: 1.95\n",
      "episode: 180, score: 4.0, mean_score: 3.66, std_score: 1.97\n",
      "episode: 185, score: 3.0, mean_score: 3.68, std_score: 1.96\n",
      "episode: 190, score: 8.0, mean_score: 3.77, std_score: 1.98\n",
      "episode: 195, score: 7.0, mean_score: 3.79, std_score: 1.99\n",
      "marking, episode: 200, score: 2.0, mean_score: 3.86, std_score: 2.18\n",
      "episode: 205, score: 7.0, mean_score: 6.20, std_score: 0.75\n",
      "episode: 210, score: 2.0, mean_score: 4.00, std_score: 2.32\n",
      "episode: 215, score: 6.0, mean_score: 4.40, std_score: 2.15\n",
      "episode: 220, score: 3.0, mean_score: 4.30, std_score: 2.08\n",
      "episode: 225, score: 4.0, mean_score: 4.20, std_score: 2.02\n",
      "episode: 230, score: 5.0, mean_score: 4.20, std_score: 1.97\n",
      "episode: 235, score: 7.0, mean_score: 4.26, std_score: 1.93\n",
      "episode: 240, score: 5.0, mean_score: 4.33, std_score: 2.03\n",
      "episode: 245, score: 7.0, mean_score: 4.51, std_score: 2.02\n",
      "episode: 250, score: 4.0, mean_score: 4.48, std_score: 1.99\n",
      "episode: 255, score: 5.0, mean_score: 4.44, std_score: 2.00\n",
      "episode: 260, score: 8.0, mean_score: 4.48, std_score: 2.07\n",
      "episode: 265, score: 3.0, mean_score: 4.45, std_score: 2.00\n",
      "episode: 270, score: 5.0, mean_score: 4.40, std_score: 2.02\n",
      "episode: 275, score: 6.0, mean_score: 4.24, std_score: 2.12\n",
      "episode: 280, score: 3.0, mean_score: 4.22, std_score: 2.14\n",
      "episode: 285, score: 5.0, mean_score: 4.26, std_score: 2.12\n",
      "episode: 290, score: 5.0, mean_score: 4.24, std_score: 2.08\n",
      "episode: 295, score: 5.0, mean_score: 4.35, std_score: 2.13\n",
      "marking, episode: 300, score: 8.0, mean_score: 4.37, std_score: 2.14\n",
      "episode: 305, score: 6.0, mean_score: 5.00, std_score: 1.10\n",
      "episode: 310, score: 5.0, mean_score: 5.40, std_score: 1.11\n",
      "episode: 315, score: 5.0, mean_score: 4.93, std_score: 1.34\n",
      "episode: 320, score: 3.0, mean_score: 4.60, std_score: 1.62\n",
      "episode: 325, score: 3.0, mean_score: 4.48, std_score: 1.72\n",
      "episode: 330, score: 4.0, mean_score: 4.40, std_score: 1.67\n",
      "episode: 335, score: 8.0, mean_score: 4.71, std_score: 1.81\n",
      "episode: 340, score: 4.0, mean_score: 4.75, std_score: 1.79\n",
      "episode: 345, score: 6.0, mean_score: 4.73, std_score: 1.70\n",
      "episode: 350, score: 5.0, mean_score: 4.86, std_score: 1.69\n",
      "episode: 355, score: 2.0, mean_score: 4.71, std_score: 1.78\n",
      "episode: 360, score: 4.0, mean_score: 4.73, std_score: 1.84\n",
      "episode: 365, score: 4.0, mean_score: 4.78, std_score: 1.82\n",
      "episode: 370, score: 6.0, mean_score: 4.64, std_score: 1.90\n",
      "episode: 375, score: 1.0, mean_score: 4.63, std_score: 1.90\n",
      "episode: 380, score: 6.0, mean_score: 4.65, std_score: 1.88\n",
      "episode: 385, score: 6.0, mean_score: 4.75, std_score: 1.91\n",
      "episode: 390, score: 5.0, mean_score: 4.89, std_score: 1.99\n",
      "episode: 395, score: 4.0, mean_score: 4.91, std_score: 1.97\n",
      "marking, episode: 400, score: 8.0, mean_score: 5.06, std_score: 2.07\n",
      "episode: 405, score: 6.0, mean_score: 5.80, std_score: 1.47\n",
      "episode: 410, score: 7.0, mean_score: 5.00, std_score: 2.00\n",
      "episode: 415, score: 6.0, mean_score: 5.07, std_score: 2.08\n",
      "episode: 420, score: 6.0, mean_score: 4.80, std_score: 2.14\n",
      "episode: 425, score: 6.0, mean_score: 5.00, std_score: 2.30\n",
      "episode: 430, score: 3.0, mean_score: 5.17, std_score: 2.72\n",
      "episode: 435, score: 8.0, mean_score: 5.17, std_score: 2.67\n",
      "episode: 440, score: 4.0, mean_score: 5.03, std_score: 2.59\n",
      "episode: 445, score: 5.0, mean_score: 4.89, std_score: 2.66\n",
      "episode: 450, score: 5.0, mean_score: 4.94, std_score: 2.55\n",
      "episode: 455, score: 6.0, mean_score: 4.93, std_score: 2.59\n",
      "episode: 460, score: 5.0, mean_score: 5.02, std_score: 2.55\n",
      "episode: 465, score: 12.0, mean_score: 5.15, std_score: 2.60\n",
      "episode: 470, score: 8.0, mean_score: 5.24, std_score: 2.57\n",
      "episode: 475, score: 4.0, mean_score: 5.17, std_score: 2.51\n",
      "episode: 480, score: 2.0, mean_score: 5.17, std_score: 2.47\n",
      "episode: 485, score: 5.0, mean_score: 5.22, std_score: 2.43\n",
      "episode: 490, score: 6.0, mean_score: 5.24, std_score: 2.43\n",
      "episode: 495, score: 7.0, mean_score: 5.33, std_score: 2.46\n",
      "marking, episode: 500, score: 11.0, mean_score: 5.40, std_score: 2.51\n",
      "episode: 505, score: 6.0, mean_score: 4.60, std_score: 1.96\n",
      "episode: 510, score: 7.0, mean_score: 5.30, std_score: 2.10\n",
      "episode: 515, score: 11.0, mean_score: 6.93, std_score: 3.21\n",
      "episode: 520, score: 7.0, mean_score: 7.05, std_score: 2.84\n",
      "episode: 525, score: 7.0, mean_score: 7.04, std_score: 2.81\n",
      "episode: 530, score: 4.0, mean_score: 6.87, std_score: 2.68\n",
      "episode: 535, score: 6.0, mean_score: 6.69, std_score: 2.57\n",
      "episode: 540, score: 8.0, mean_score: 6.90, std_score: 2.48\n",
      "episode: 545, score: 3.0, mean_score: 6.87, std_score: 2.71\n",
      "episode: 550, score: 5.0, mean_score: 6.84, std_score: 2.62\n",
      "episode: 555, score: 4.0, mean_score: 6.65, std_score: 2.68\n",
      "episode: 560, score: 8.0, mean_score: 6.60, std_score: 2.75\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "seed = 742\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "marking=[]\n",
    "episodeScores=[]\n",
    "stepBuffer = deque(maxlen=n_step_return)\n",
    "#precalculate GAMMA n step return values\n",
    "gamma_nsteps = [GAMMA ** i for i in range(n_step_return+1)]\n",
    "sequenceBuffer = []\n",
    "recurrentBuffer = []\n",
    "frames = 0\n",
    "for n_episode in tqdm(range(num_episodes+1)):\n",
    "    # print('New Episode')\n",
    "    sequenceBuffer = []\n",
    "    recurrentBuffer = []\n",
    "    score=0\n",
    "    # Initialize the environment and state\n",
    "    temp = env.reset()\n",
    "    state = get_screen(temp)\n",
    "    lives = env.ale.lives\n",
    "    while True:\n",
    "        recurrentBuffer.append(policy_net.getState())\n",
    "        # print(recurrentBuffer[-1][0].shape,recurrentBuffer[-1][1].shape)\n",
    "        # Select and perform an action\n",
    "        action = select_action(state,n_episode,recurrentBuffer[-1])\n",
    "\n",
    "        obvs, reward, done, info = env.step(action.item())\n",
    "        score+=reward\n",
    "\n",
    "        #Reward clipping and negative reward on death\n",
    "        if info['ale.lives'] != lives:\n",
    "            reward-=1\n",
    "            lives = info['ale.lives']\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        reward = torch.tensor([reward])\n",
    "\n",
    "        # Observe new state\n",
    "        next_state = get_screen(obvs)\n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        stepBuffer.append(Transition(state,action,next_state,reward,torch.tensor([float(done)])))\n",
    "        if len(stepBuffer) == stepBuffer.maxlen:\n",
    "            reward_nstep = sum([gamma_nsteps[-(i+2)] * stepBuffer[i].reward for i in range(n_step_return)])\n",
    "            sequenceBuffer.append(Transition(stepBuffer[0].state,stepBuffer[0].action,stepBuffer[0].next_state,reward_nstep,torch.tensor([float(done)])))\n",
    "        if (len (sequenceBuffer)) == n_sequence+n_burn_in:\n",
    "            memory.push(Sequence(sequenceBuffer,recurrentBuffer[0]))\n",
    "            #chop sequence and recurrent buffers\n",
    "            # print(len(sequenceBuffer[-(n_burn_in+n_overlap):]),len(recurrentBuffer[-(n_burn_in+n_overlap+n_step_return-1):]))\n",
    "            sequenceBuffer = sequenceBuffer[-(n_burn_in+n_overlap):]\n",
    "            recurrentBuffer = recurrentBuffer[-(n_burn_in+n_overlap+n_step_return):]\n",
    "            # print(recurrentBuffer[0][0].shape,recurrentBuffer[0][1].shape)\n",
    "        elif done and len(sequenceBuffer) > n_burn_in+n_overlap+n_step_return:\n",
    "            # case of we're done so we push the rest of the sequence to memory\n",
    "            # print(recurrentBuffer[0][0].shape,recurrentBuffer[0][1].shape)\n",
    "            memory.push(Sequence(sequenceBuffer,recurrentBuffer[0]))\n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if frames % TARGET_UPDATE == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if frames % POLICY_UPDATE == 0:\n",
    "            tmpState = policy_net.getState(False)\n",
    "            optimize_model(n_episode)\n",
    "            policy_net.setState(tmpState,device,False)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        frames+=1\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            stepBuffer.clear()\n",
    "\n",
    "            break\n",
    "\n",
    "    episodeScores.append(score)\n",
    "    # do not change lines 44-48 here, they are for marking the submission log\n",
    "    marking.append(score)\n",
    "    if n_episode%100 == 0:\n",
    "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
    "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
    "        marking = []\n",
    "\n",
    "    if n_episode % OUTPUT_FREQUENCY == 0:\n",
    "        if len(marking)>0:\n",
    "            print(\"episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
    "                n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "# env.render()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trend Graphs\n",
    "episodeScoresEnum = sorted(enumerate(episodeScores),reverse=True,key=lambda x:x[1])\n",
    "print('Top 5 scores overall')\n",
    "print(episodeScoresEnum[:5])\n",
    "tempScores = sorted([(i,j) for i,j in enumerate(episodeScores) if i%video_every==0],key=lambda x:x[1],reverse=True)\n",
    "print('Top videos:')\n",
    "print(tempScores[:10])\n",
    "print('Bottom Videos')\n",
    "print(tempScores[-10:][::-1])\n",
    "print('Total Frames Trained on: ',frames)\n",
    "plt.plot(range(len(episodeScores)),episodeScores)\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "averages = []\n",
    "tempAverage = 0\n",
    "for i in enumerate(episodeScores):\n",
    "    tempAverage+=i[1]\n",
    "    if i[0]%30==0:\n",
    "        averages.append(tempAverage/30)\n",
    "        tempAverage=0\n",
    "plt.plot(range(len(averages)),averages)\n",
    "plt.ylabel('Average score')\n",
    "plt.xlabel('per 30 episodes')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if num_episodes>100:\n",
    "    averages = []\n",
    "    tempAverage = 0\n",
    "    for i in enumerate(episodeScores):\n",
    "        tempAverage+=i[1]\n",
    "        if i[0]%100==0:\n",
    "            averages.append(tempAverage/100)\n",
    "            tempAverage=0\n",
    "    plt.plot(range(len(averages)),averages)\n",
    "    plt.ylabel('Average score')\n",
    "    plt.xlabel('per 100 episodes')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}